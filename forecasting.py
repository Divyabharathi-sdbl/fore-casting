# -*- coding: utf-8 -*-
"""forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ioPbFBGsgYytnEMuiNDfifwJnCK0UGuM
"""

#  install packages
!pip install -q numpy pandas matplotlib scikit-learn torch tqdm seaborn
#install libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm


import torch
import torch.nn as nn


from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error


# create folders
os.makedirs('data', exist_ok=True)
os.makedirs('results', exist_ok=True)
os.makedirs('results/model_checkpoints', exist_ok=True)
os.makedirs('results/attention_maps', exist_ok=True)


# metrics
def mape(y_true, y_pred, eps=1e-8):
    return np.mean(np.abs((y_true - y_pred) / (np.clip(np.abs(y_true), eps, None)))) * 100.0


def evaluate_metrics(y_true, y_pred):
    # y_true, y_pred: [N, H, F]
    true_r = y_true.reshape(-1, y_true.shape[-1])
    pred_r = y_pred.reshape(-1, y_pred.shape[-1])
    maes = mean_absolute_error(true_r, pred_r)
    rmses = np.sqrt(mean_squared_error(true_r, pred_r))
    mapes = mape(true_r, pred_r)
    return {'MAE': maes, 'RMSE': rmses, 'MAPE': mapes}

import numpy as np
import pandas as pd


def generate_multivariate(n_points=6000, n_features=5, seed=0):
    rng = np.random.RandomState(seed)
    t = np.arange(n_points)
    data = []
    for f in range(n_features):
        freq = rng.uniform(0.0005, 0.01)
        phase = rng.uniform(0, 2*np.pi)
        seasonal = np.sin(2*np.pi*freq*t + phase)
        trend = 0.0002 * t * rng.uniform(-1, 1)
        noise = 0.2 * rng.normal(size=n_points)
        coupling = 0.0
        if f>0:
            coupling = 0.1 * (np.roll(data[-1], 1) - np.mean(data[-1]))
        series = (1 + 0.5*f) * seasonal + trend + noise + coupling
        spikes = (rng.rand(n_points) < 0.001).astype(float) * rng.normal(0, 5, size=n_points)
        series += spikes
        data.append(series)
    arr = np.vstack(data).T
    return arr

# generate and save
arr = generate_multivariate(n_points=6000, n_features=5, seed=42)
df = pd.DataFrame(arr, columns=[f'f{i}' for i in range(arr.shape[1])])
df.to_csv('data/generated_timeseries.csv', index=False)
print('Saved data/generated_timeseries.csv — shape:', df.shape)

df.head()

import numpy as np
from sklearn.preprocessing import StandardScaler
import torch
from torch.utils.data import TensorDataset, DataLoader


# load
data = pd.read_csv('data/generated_timeseries.csv').values


# parameters
INPUT_LEN = 96
HORIZON = 24
BATCH_SIZE = 64


# scale whole series (we will re-fit scalers per rolling fold later for strictness; for demo use global scaler)
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)


# create windows
X, Y = [], []
for i in range(len(data_scaled) - INPUT_LEN - HORIZON + 1):
    X.append(data_scaled[i:i+INPUT_LEN])
    Y.append(data_scaled[i+INPUT_LEN:i+INPUT_LEN+HORIZON])
X = np.array(X)
Y = np.array(Y)


print('X shape, Y shape:', X.shape, Y.shape)


# create dataloader for training demo (we'll use full dataset in this runnable demo)
tensor_x = torch.tensor(X, dtype=torch.float32)
tensor_y = torch.tensor(Y, dtype=torch.float32)


dataset = TensorDataset(tensor_x, tensor_y)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

class LSTMBaseline(nn.Module):
    def __init__(self, n_features, hidden_size):
        super().__init__()
        self.lstm = nn.LSTM(n_features, hidden_size, batch_first=True)
        self.linear = nn.Linear(hidden_size, n_features * HORIZON)
        self.n_features = n_features

    def forward(self, x):
        # x shape: (batch_size, input_len, n_features)
        lstm_out, _ = self.lstm(x)
        # Take the output of the last time step
        last_time_step_out = lstm_out[:, -1, :]
        # Pass through linear layer to predict HORIZON steps for n_features
        predictions = self.linear(last_time_step_out)
        # Reshape to (batch_size, HORIZON, n_features)
        predictions = predictions.view(-1, HORIZON, self.n_features)
        return predictions

# instantiate
n_features = data.shape[1]
model_lstm = LSTMBaseline(n_features=n_features, hidden_size=128)
optimizer = torch.optim.Adam(model_lstm.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()


# quick small training for demo (5 epochs) — for submission do 30-50 epochs with early stopping
EPOCHS = 5
for epoch in range(EPOCHS):
    model_lstm.train()
    total_loss = 0
    for xb, yb in dataloader:
        optimizer.zero_grad()
        pred = model_lstm(xb)
        loss = loss_fn(pred, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * xb.size(0)
    print(f'Epoch {epoch+1}/{EPOCHS} Loss: {total_loss/len(dataset):.6f}')


# save
torch.save(model_lstm.state_dict(), 'results/model_checkpoints/lstm_baseline.pth')
print('Saved LSTM checkpoint')

import torch.nn.functional as F


class Encoder(nn.Module):
    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True, dropout=dropout)

    def forward(self, x):
        outputs, (h, c) = self.lstm(x)
        return outputs, (h, c)


class BahdanauAttention(nn.Module):
    def __init__(self, enc_hidden, dec_hidden):
        super().__init__()
        self.W1 = nn.Linear(enc_hidden, dec_hidden)
        self.W2 = nn.Linear(dec_hidden, dec_hidden)
        self.V = nn.Linear(dec_hidden, 1)

    def forward(self, enc_outputs, dec_hidden):
        # enc_outputs: [B, src_len, enc_hidden]
        # dec_hidden: [B, dec_hidden]
        seq_len = enc_outputs.size(1)
        dec_hidden_exp = dec_hidden.unsqueeze(1).repeat(1, seq_len, 1)
        score = self.V(torch.tanh(self.W1(enc_outputs) + self.W2(dec_hidden_exp)))  # [B, seq_len, 1]
        attn_weights = torch.softmax(score, dim=1)  # [B, seq_len, 1]
        context = torch.sum(attn_weights * enc_outputs, dim=1)  # [B, enc_hidden]
        return context, attn_weights.squeeze(-1)


class Decoder(nn.Module):
    def __init__(self, output_size, hidden_size, enc_hidden, n_layers=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.attention = BahdanauAttention(enc_hidden, hidden_size)
        self.lstm_cell = nn.LSTMCell(output_size + enc_hidden, hidden_size)
        self.fc_out = nn.Linear(hidden_size, output_size)

    def forward_step(self, input_step, last_hidden, last_cell, enc_outputs):
        context, attn_weights = self.attention(enc_outputs, last_hidden)
        lstm_in = torch.cat([input_step, context], dim=-1)
        h_next, c_next = self.lstm_cell(lstm_in, (last_hidden, last_cell))
        out = self.fc_out(h_next)
        return out, h_next, c_next, attn_weights


class Seq2SeqAttention(nn.Module):
    def __init__(self, n_features, hidden_size=128, output_len=HORIZON):
        super().__init__()
        self.encoder = Encoder(n_features, hidden_size)
        self.decoder = Decoder(n_features, hidden_size, enc_hidden=hidden_size)
        self.output_len = output_len
        self.n_features = n_features

    def forward(self, src, tgt=None, teacher_forcing_ratio=0.5):
        # src: [B, src_len, F]
        batch_size = src.size(0)
        enc_outputs, (h, c) = self.encoder(src)
        dec_hidden = h[-1]
        dec_cell = c[-1]
        if tgt is not None:
            target_len = tgt.size(1)
        else:
            target_len = self.output_len
        outputs = []
        attn_maps = []
        prev = src[:, -1, :]
        for t in range(target_len):
            out, dec_hidden, dec_cell, attn_weights = self.decoder.forward_step(prev, dec_hidden, dec_cell, enc_outputs)
            outputs.append(out.unsqueeze(1))
            attn_maps.append(attn_weights.unsqueeze(1))
            if tgt is not None and torch.rand(1).item() < teacher_forcing_ratio:
                prev = tgt[:, t, :]
            else:
                prev = out
        return torch.cat(outputs, dim=1), torch.cat(attn_maps, dim=1)

# Instantiate the model
model_attn = Seq2SeqAttention(n_features=n_features, hidden_size=128, output_len=HORIZON)
# save
torch.save(model_attn.state_dict(), 'results/model_checkpoints/seq2seq_attention.pth')
print('Saved attention model checkpoint')

# quick evaluation using last 10% as test set
split_idx = int(0.9 * len(X))
X_train, Y_train = X[:split_idx], Y[:split_idx]
X_test, Y_test = X[split_idx:], Y[split_idx:]


# convert to tensors
X_test_t = torch.tensor(X_test, dtype=torch.float32)
Y_test_t = torch.tensor(Y_test, dtype=torch.float32)


# LSTM preds
model_lstm.eval()
with torch.no_grad():
    preds_lstm = model_lstm(X_test_t).numpy()


# Attn preds
model_attn.eval()
with torch.no_grad():
    preds_attn, attn_map = model_attn(X_test_t)
    preds_attn = preds_attn.numpy()
    attn_map = attn_map.numpy()


# inverse transform
# reshape to flat then inverse transform, careful with scaler
nt = preds_lstm.reshape(-1, n_features)
orig_pred_lstm = scaler.inverse_transform(nt).reshape(preds_lstm.shape)
nt2 = preds_attn.reshape(-1, n_features)
orig_pred_attn = scaler.inverse_transform(nt2).reshape(preds_attn.shape)

# Choose a test sample
idx = 5
feature = 0  # choose feature to plot

# Inverse transform the true future values
orig_true = scaler.inverse_transform(Y_test.reshape(-1, n_features)).reshape(Y_test.shape)

# Extract data
obs = scaler.inverse_transform(X_test[idx])
true_future = orig_true[idx]
pred_lstm = orig_pred_lstm[idx]
pred_attn = orig_pred_attn[idx]

obs_len = obs.shape[0]
h = HORIZON

plt.figure(figsize=(12, 5))

# Observed past
plt.plot(range(obs_len), obs[:, feature], label="Observed (Past)", linewidth=2)

# True future
plt.plot(range(obs_len, obs_len+h), true_future[:, feature], label="True Future", linewidth=2)

# LSTM prediction
plt.plot(range(obs_len, obs_len+h), pred_lstm[:, feature], label="LSTM Prediction", linestyle="--")

# Attention model prediction
plt.plot(range(obs_len, obs_len+h), pred_attn[:, feature], label="Attention Prediction", linestyle="--")

plt.title(f"Prediction Comparison (Feature {feature})")
plt.xlabel("Time Steps")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.show()

# attn_map shape: [N_test, HORIZON, INPUT_LEN]
sample_attn = attn_map[idx]  # choose same idx as above

plt.figure(figsize=(10, 6))
plt.imshow(sample_attn, aspect='auto', origin='lower', cmap='viridis')

plt.colorbar(label="Attention Weight")
plt.title(f"Attention Heatmap (Sample {idx})")
plt.xlabel("Encoder Time Steps (Input Window)")
plt.ylabel("Decoder Time Steps (Forecast Horizon)")
plt.show()

num_features = orig_true.shape[2]

plt.figure(figsize=(14, 10))

for f in range(num_features):
    plt.subplot(num_features, 1, f+1)
    plt.plot(true_future[:, f], label="True", linewidth=2)
    plt.plot(pred_attn[:, f], label="Attention Pred", linestyle='--')
    plt.plot(pred_lstm[:, f], label="LSTM Pred", linestyle=':')
    plt.ylabel(f"Feature {f}")
    plt.grid(True)
    if f == 0:
        plt.legend()

plt.suptitle("Prediction Comparison Across All Features")
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
plt.imshow(sample_attn, aspect='auto', origin='lower', cmap='viridis')
plt.colorbar()
plt.title("Attention Heatmap")
plt.savefig("results/attention_maps/attention_map_sample.png", dpi=300, bbox_inches='tight')
plt.show()

print("Saved → results/attention_maps/attention_map_sample.png")